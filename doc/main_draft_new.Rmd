---
title: 'Group 3 Project on Optical character recognition (OCR)'
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
    code_folding: hide
---

Course information: 
Jing Wu
GU4243/GR5243: Applied Data Science

# Introduction {-}

Optical character recognition (OCR) is the process of converting scanned images of machine printed or
handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). As shown in Figure 1, the data *workflow* in a typical OCR system consists of three major stages:

* Pre-processing

* Word recognition

* Post-processing

![](../figs/ocr_flowchart.png) 

To recieve input data for the project, raw scanned images were processed through the first two steps are relying on the [Tessearct OCR machine](https://en.wikipedia.org/wiki/Tesseract_(software)). R package tutorial can be found [here](https://www.r-bloggers.com/the-new-tesseract-package-high-quality-ocr-in-r/). 

This project is aimed to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*. There is a total of 100 pairs of files representing ground truth and text output of OCR Machine.

# Step 1 - Load libraries and source code

```{r, warning=FALSE, message = FALSE}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
  ## devtools is required
  library(devtools)
  install_github("trinker/pacman")
}

pacman::p_load(knitr, readr, stringr, tesseract, vecsets)

library(stringr)
library(topicmodels)
library(tm)
library(tidytext)
library(dplyr)
library(ldatuning)
library(SnowballC)  
library(NLP)
library(tm)
library(Hmisc)

source('../lib/Detection.R')
source('../lib/candidate_vector.R')
source('../lib/dtm.R')
source('../lib/get_probability_word.R')
source('../lib/candidate_word_score.R')
source('../lib/confusion_matrix.R')

#outputs steps 2,3

load("../output/tesseract_error.Rdata") #ocr.error
load("../output/tesseract_correct.Rdata") #ocr.correct
load("../output/tesseract_full_data.Rdata") #ocr.all.text
load("../output/ground_truth_full_data.Rdata") #ground.truth.all.text
load("../output/candidates_list.Rdata") #candidates.list
load("../output/candidates_list_nonempty.Rdata") #candidates.list.nonempty
load("../output/ocr_true_error_nonempty.Rdata") #ocr.true.error.nonempty

```

# Step 2 - Error detection

Error detection is the first step of post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words*. For that we use [Rule-based techniques](http://webpages.ursinus.edu/akontostathis/KulpKontostathisFinal.pdf), specifically rules are in the section 2.2. The code takes 100 documents, creates a single big corpus, which is splitted into two: one contains correct text elements, the other contains erroneous text elements. Only inuque tockens are left for alanysis to reduce the amount of computation.

```{r}
file.name.vector <- list.files("../data/ground_truth")

ground.truth.all = c()
for (doc in 1:length(file.name.vector)){
  ground.truth.text.vector = readLines(paste("../data/ground_truth/", file.name.vector[doc],sep=""))
  ground.truth.all = c(ground.truth.all, ground.truth.text.vector)
  ground.truth.all.text = paste(ground.truth.all, collapse = " ")
}

save(ground.truth.all.text, file = "../output/ground_truth_full_data.Rdata")

ocr.all = c()
for (doc in 1:length(file.name.vector)){
  ocr.text.vector = readLines(paste("../data/tesseract/", file.name.vector[doc],sep =""))
  ocr.all = c(ocr.all, ocr.text.vector)
  ocr.all.text = paste(ocr.all, collapse = " ")
}

save(ocr.all.text, file = "../output/tesseract_full_data.Rdata")

ocr.tokens = str_split(ocr.all.text," ")
ocr.tokens.unique = unique(ocr.tokens[[1]])
ocr.boolean.if.clean = lapply(ocr.tokens.unique,Detection)
ocr.boolean.if.clean = unlist(ocr.boolean.if.clean)
ocr.error = ocr.tokens.unique[!ocr.boolean.if.clean] #9418
ocr.correct = ocr.tokens.unique[ocr.boolean.if.clean]

save(ocr.error, file = "../output/tesseract_error.Rdata")
save(ocr.correct, file = "../output/tesseract_correct.Rdata")
```
# Step 3 - Error correction

To correct errors detected in the previous step, we followed the paper [Topic models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4377099). Error correction algorithm considered in the paper consists of two models: a topic model that provides information about word probabilities and OCR model that represents the probability of character errors. For each error word we compute a vector of candidates to substitute the erroneous word and pick the one that provides the best score according to the formula proposed in the paper.

```{r}
load("../output/tesseract_error.Rdata") #ocr.error
load("../output/tesseract_correct.Rdata") #ocr.correct
load("../output/tesseract_full_data.Rdata") #ocr.all.text
load("../output/ground_truth_full_data.Rdata") #ground.truth.all.text

dictionary = unique(unlist(strsplit(ground.truth.all.text, split = " ")))

#There is 15% intersection between dictionary and error vector. Removing those.

not.in.dictionary.boolean = which(ocr.error %nin% dictionary)
ocr.true.error = ocr.error[not.in.dictionary.boolean] #8046

#using of grid provides many-fold time optimivation vs for-loop (0.07sec vs 10sec/error.word)
tic  = Sys.time()
candidates.list = lapply(ocr.true.error, get.candidate.vector, dictionary = dictionary)
toc = Sys.time()
toc-tic 
ocr.true.error.nonempty = ocr.true.error[lapply(candidates.list, length)>0]
length(ocr.true.error.nonempty) #3875
candidates.list.nonempty = candidates.list[lapply(candidates.list, length)>0]
length(candidates.list.nonempty) #3875

save(candidates.list, file = "../output/candidates_list.Rdata")
save(candidates.list.nonempty, file = "../output/candidates_list_nonempty.Rdata")
save(ocr.true.error.nonempty, file = "../output/ocr_true_error_nonempty.Rdata")
```

```{r}
library(R.oo)
cm = confusion_count_num("../data/ground_truth/group1_00000005.txt", "../data/trasserect/group1_00000005.txt")
```


```{r}
gt <- Corpus(DirSource("../data/ground_truth"), readerControl = list(language = "english"))
gt <- tm_map(gt, PlainTextDocument)
gt <- tm_map(gt, stripWhitespace)
gt <- tm_map(gt, tolower)
gt <-tm_map(gt,removeNumbers)
gt <-tm_map(gt,removePunctuation)
gt <- tm_map(gt, removeWords, stopwords("english"))
tm_map(gt, stemDocument)
dtm <- DocumentTermMatrix(gt)

result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)

```

LDA is trained on the whole text data available at once.
```{r}
n.topics = 12 #optimal number of clusters based on the result above
dtm.ground.truth = get.dtm(ground.truth.all.text) 
lda = LDA(dtm.ground.truth, k = n.topics, method = "VEM", control = list(seed = 2019))
```


```{r}
#P(t_k) computed by applying the trained topic model to the correctly recognized words in the document. 
dtm.ocr = get.dtm(ocr.all.text)
probability.topic.term = posterior(lda, dtm.ocr) 
probability.topic = probability.topic.term$topics #vector 12 probabilities
beta.matrix = tidy(lda, matrix = "beta") #restart R if "Error: No tidy method for objects of class LDA_VEM", this is package bug
```

```{r}
#Temp
#P(w|t_k) is probability of a word from candidate vector given the topic, computed using beta matrix.

tic  = Sys.time()
list.scores.word = c()
for (i in 1:length(candidates.list.nonempty)){
  candidate.list = lapply(candidates.list.nonempty[[i]], get.probability.word, beta.matrix = beta.matrix)
  score = sum(candidate.matrix$beta * probability.topic)
  list.scores.word[[length(list.scores.word) + 1]] = candidate.list
}

toc = Sys.time()
toc-tic #2.3 min

length(list.scores.word)
sum(list.scores.word[[5]][[2]]*probability.topic)

```

```{r}
#temp
#P(w) = sum(P(w|t_k)*P(t_k))
#it is best to compute score at once, need cm prob score.
list.scores = c()
for (i in 1:length(list.scores.word)){
  for (j in 1:length(list.scores.word[[i]]))
  score = sum(list.scores.word[[i]][[j]] * probability.topic)
  list.scores[[length(list.scores) + 1]] = score
}

length(list.scores)
```

```{r}
confusion.prob = 1
candidates.list.nonempty[[15]]
candidate.word.score = lapply(candidates.list.nonempty[[15]], candidate.word.score, probability.topic = probability.topic, confusion.prob = confusion.prob, beta.matrix = beta.matrix)
candidate.word.score.max = candidates.list.nonempty[[15]][which.max(candidate.word.score)]

ocr.true.error.nonempty[[15]]
text  = c("i", "worklng", "worklng", "a", "lot")

corrected.text = gsub(ocr.true.error.nonempty[[15]], candidate.word.score.max, as.character(text))
ocr.all.text[1:100]
```

