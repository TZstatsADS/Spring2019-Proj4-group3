---
title: 'Group 3 Project on Optical character recognition (OCR)'
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
    code_folding: hide
---

Course information: 
Jing Wu
GU4243/GR5243: Applied Data Science

# Introduction {-}

Optical character recognition (OCR) is the process of converting scanned images of machine printed or
handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). As shown in Figure 1, the data *workflow* in a typical OCR system consists of three major stages:

* Pre-processing

* Word recognition

* Post-processing

![](../figs/ocr_flowchart.png) 

To recieve input data for the project, raw scanned images were processed through the first two steps are relying on the [Tessearct OCR machine](https://en.wikipedia.org/wiki/Tesseract_(software)). R package tutorial can be found [here](https://www.r-bloggers.com/the-new-tesseract-package-high-quality-ocr-in-r/). 

This project is aimed to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*. There is a total of 100 pairs of files representing ground truth and text output of OCR Machine.

# Step 1 - Load libraries and source code

```{r, warning=FALSE, message = FALSE}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
  ## devtools is required
  library(devtools)
  install_github("trinker/pacman")
}

pacman::p_load(knitr, readr, stringr, tesseract, vecsets)

library(stringr)
library(topicmodels)
library(tm)
library(tidytext)
library(dplyr)

source('../lib/Detection.R')
#source('../lib/LDA.R')

```

# Step 2 - Error detection

Error detection is the first step of post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words*. For that we use [Rule-based techniques](http://webpages.ursinus.edu/akontostathis/KulpKontostathisFinal.pdf), specifically rules are in the section 2.2. The code takes 100 documents and splits them into two lists sized 100 each: one contains correct text elements, the other contains erroneous text elements. 

```{r}
file.name.vector <- list.files("../data/ground_truth")

ground.truth.all = c()
for (doc in 1:length(file.name.vector)){
  ground.truth.text.vector = readLines(paste("../data/ground_truth/", file.name.vector[doc],sep=""))
  ground.truth.all = c(ground.truth.all, ground.truth.text.vector)
}

save(ground.truth.all, file = "../output/ground_truth_full_data.Rdata")

ocr.all = c()
for (doc in 1:length(file.name.vector)){
  tesseract.text.vector = readLines(paste("../data/tesseract/", file.name.vector[doc],sep=""))
  tesseract.text.unit = paste(tesseract.text.vector, collapse = " ")
  ocr.all = c(ocr.all, tesseract.text.unit)
}

save(ocr.all, file = "../output/tesseract_full_data.Rdata")

tesseract.error.all = c()
tesseract.correct.all = c()
for (doc in 1:length(ocr.all)){
  tesseract.tokens = str_split(ocr.all," ")[[doc]]
  tesseract.boolean.if.clean = unlist(lapply(tesseract.tokens,Detection)) 
  tesseract.error = tesseract.tokens[!tesseract.boolean.if.clean]
  tesseract.error.unit = paste(tesseract.error, collapse = " ")
  tesseract.correct = tesseract.tokens[tesseract.boolean.if.clean]
  tesseract.correct.unit = paste(tesseract.correct, collapse = " ")
  tesseract.error.all = c(tesseract.error.all, tesseract.error.unit)
  tesseract.correct.all = c(tesseract.correct.all, tesseract.correct.unit)
}

save(tesseract.error.all, file = "../output/tesseract_error_100_docs.Rdata")
save(tesseract.correct.all, file = "../output/tesseract_correct_100_docs.Rdata")
```
# Step 4 - Error correction

To correct errors detected in the previous step, we followed the paper [Topic models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4377099). Error correction algorithm considered in the paper consists of two models: a topic model that provides information about word probabilities and OCR model that represents the probability of character errors. For each error word we compute a vector of candidates to substitute the erroneous word and pick the one that provides the best score according to the formula proposed in the paper.

```{r}
load("../output/tesseract_error_100_docs.Rdata") #tesseract.error.all
load("../output/tesseract_correct_100_docs.Rdata") #tesseract.correct.all
load("../output/tesseract_full_data.Rdata") #ocr.all
load("../output/ground_truth_full_data.Rdata") #ground.truth.all

#tressarect.correct.words = tesseract.correct.all
#tressarect.correct.text = paste0(tressarect.correct.words, collapse = " ")

dictionary = unique(unlist(strsplit(ground.truth.all, split = " ")))
get.candidate.vector(error.word, dictionary)

list.tokens.error = c()
for (doc in 1:length(tesseract.correct.all)){
  token.error.word = str_split(tesseract.error.all[doc], " ") 
  list.tokens.error[[length(list.tokens.error) + 1]] = token.error.word
}

#!!!!time consuming, even with length list.tokens.error[1]
list.candidates = c()
for (doc in 1:length(list.tokens.error)){
  for(word in 1:length(list.tokens.error[[doc]][[1]])){
    candidates.list = get.candidate.vector(list.tokens.error[[doc]][[1]][word], dictionary)
    list.candidates[[length(list.candidates) + 1]] = candidates.list
  }
}

#example
candidates.list.1 = get.candidate.vector(list.tokens.error[[1]][[1]][15], dictionary)
```

LDA is trained on the whole text data available at once.
```{r}
n.topics = 12 #based in cross-validation
dtm.ground.truth = get.dtm(ground.truth.all) #sparsity = 100% as non unique entries are removed (~7%)
lda = LDA(dtm.ground.truth, k = n.topics, method = "VEM", control = list(seed = 2019))
```
```{r}
dtm.tresseract.1 = get.dtm(tesseract.correct.all[1])

#P(t_k) computed by applying the trained topic model to the correctly recognized words in the document. 
probability.topic.term = posterior(lda, dtm.tresseract.1) 
score.gamma = probability.topic.term$topics
```

```{r}
#P(w|t_k) is probability of a word from candidate vector given the topic, computed using beta matrix.
beta.matrix = tidy(lda, matrix = "beta")
beta.matrices = c()
for (i in 1:length(candidates.list.1)){
  candidate.matrix = beta.matrix %>% filter(term == candidates.list.1[i]) %>% arrange(topic)
  beta.matrices[[length(beta.matrices) + 1]] = candidate.matrix
}
```

```{r}
#P(w) = sum(P(w|t_k)*P(t_k))

list.scores = c()
for (i in 1:length(score.beta)){
  score = sum(beta.matrices[[i]][3] * score.gamma)
  list.scores = c(list.scores, score)
}
```

